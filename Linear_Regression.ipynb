{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Linear Regression**\n",
        "\n",
        "A linear regression model with both L1 and L2 regularization will be implemented. The class LinearRegression will have the following API:\n",
        "\n",
        "* `__init__(alpha, tol, max_iter, theta_init, penalty, lambd)`\n",
        "* `compute_cost(theta, X, y)`\n",
        "* `compute_gradient(theta, X, y)`\n",
        "* `fit(X, y)`\n",
        "* `has_converged(theta_old, theta_new)`\n",
        "* `predict(X)`\n",
        "\n",
        "### **Cost Function**\n",
        "\n",
        "The cost function computes the scalar cost for a given $\\theta$ vector. \n",
        "\n",
        "> $\n",
        "\\mathcal{L}({\\theta}) = \\frac{1}{N}\\sum_{i =1}^N (h_{{\\theta}}({x}_i) - y_i)^2\n",
        "$\n",
        "\n",
        "where\n",
        "\n",
        "> $h_{{\\theta}}({x}_i) = \\theta^Tx_i$\n",
        "\n",
        "L1 Regularization Loss:\n",
        ">$\n",
        "\\mathcal{L_1}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda\\sum_{j = 1}^D  |{\\theta}_j|\n",
        "$\n",
        "\n",
        "L2 Regularization Loss:\n",
        ">$\n",
        "\\mathcal{L_2}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda\\sum_{j = 1}^D  {\\theta}_j^2 \n",
        "$\n",
        "\n",
        "$N$ is the number of training samples and $D$ is the number of features (excluding the intercept term). $\\theta$ is a $D + 1$ dimensional vector, with the first element being the intercept term. "
      ],
      "metadata": {
        "id": "xfs6aWUhmWJT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb-WLp5Z-cdy"
      },
      "source": [
        "import random \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import *\n",
        "np.random.seed(42)\n",
        "\n",
        "import dill\n",
        "import base64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression:\n",
        "\n",
        "    \"\"\"\n",
        "    Linear Regression\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha: float, default=0.01\n",
        "        Learning rate\n",
        "    tol : float, default=0.0001\n",
        "        Tolerance for stopping criteria\n",
        "    max_iter : int, default=10000\n",
        "        Maximum number of iterations of gradient descent\n",
        "    theta_init: None (or) numpy.ndarray of shape (D + 1,)\n",
        "        The initial weights; if None, all weights will be zero by default\n",
        "    penalty : string, default = None\n",
        "        The type of regularization. The other acceptable options are l1 and l2\n",
        "    lambd : float, default = 1.0\n",
        "        The parameter regularisation constant (i.e. lambda)\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    theta_ : numpy.ndarray of shape (D + 1,)\n",
        "        The value of the coefficients after gradient descent has converged\n",
        "        or the number of iterations hit the maximum limit\n",
        "    hist_theta_ : numpy.ndarray of shape (num_iter, D + 1) where num_iter is the number of gradient descent iterations\n",
        "        Stores theta_ after every gradient descent iteration\n",
        "    hist_cost_ : numpy.ndarray of shape (num_iter,) where num_iter is the number of gradient descent iterations\n",
        "        Stores cost after every gradient descent iteration\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, alpha = 0.01, tol=1e-4, max_iter = 100, theta_init = None, penalty = None, lambd = 0):\n",
        "        \n",
        "        self.alpha = alpha\n",
        "        self.theta_init = theta_init\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.penalty = penalty\n",
        "        self.lambd = lambd\n",
        "\n",
        "        self.theta_ = None\n",
        "        self.hist_cost_ = None\n",
        "        self.hist_theta_ = None\n",
        "    \n",
        "    def compute_cost(self, theta, X, y):\n",
        "    \n",
        "        \"\"\"\n",
        "        Compute the cost/objective function.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta: numpy.ndarray of shape (D + 1,)\n",
        "            The coefficients\n",
        "        X: numpy.ndarray of shape (N, D + 1)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        cost: float\n",
        "            The cost as a scalar value\n",
        "        \"\"\"\n",
        "        h = np.matmul(X, theta) # (N, )\n",
        "        L = np.mean(np.square(h - y)) # scalar\n",
        "        L1 = L + self.lambd*np.sum(np.abs(theta[1:]))\n",
        "        L2 = L + self.lambd*np.sum(np.square(theta[1:]))\n",
        "        if self.penalty == 'l1': return L1\n",
        "        elif self.penalty == 'l2': return L2\n",
        "\n",
        "\n",
        "    def compute_gradient(self, theta, X, y):\n",
        "    \n",
        "        \"\"\"\n",
        "        Compute the gradient of the cost function.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta: numpy.ndarray of shape (D + 1,)\n",
        "            The coefficients\n",
        "        X: numpy.ndarray of shape (N, D + 1)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        gradient: numpy.ndarray of shape (D + 1,)\n",
        "            The gradient values\n",
        "        \"\"\"\n",
        "        \n",
        "        h = np.matmul(X, theta) # (N, D+1)\n",
        "        delta_y = h - y # (N, )\n",
        "        dL = 2*np.matmul(X.T, delta_y)/len(y) # (D+1, )\n",
        "        dL1 = dL + np.insert(self.lambd*np.sign(theta[1:]),0,0)  # (D+1, )\n",
        "        dL2 = dL + np.insert(2*self.lambd*theta[1:],0,0) # (D+1, )\n",
        "        if self.penalty == 'l1': return dL1\n",
        "        elif self.penalty == 'l2': return dL2\n",
        "        else: return dL\n",
        "\n",
        "    def has_converged(self, theta_old, theta_new):\n",
        "\n",
        "        \"\"\"\n",
        "        Return whether gradient descent has converged.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta_old: numpy.ndarray of shape (D + 1,)\n",
        "            The weights prior to the update by gradient descent\n",
        "        theta_new: numpy.ndarray of shape (D + 1,)\n",
        "            The weights after the update by gradient descent\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        converged: bool\n",
        "            Whether gradient descent converged or not\n",
        "        \"\"\"\n",
        "\n",
        "        norm_2 = np.linalg.norm((theta_old - theta_new),2)\n",
        "        if norm_2 <= self.tol: return True\n",
        "        else: return False\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the coefficients using gradient descent and store them as theta_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Nothing\n",
        "        \"\"\"\n",
        "\n",
        "        N, D = X.shape\n",
        "        ones_col = np.ones((N, 1))\n",
        "        X = np.hstack((ones_col, X))\n",
        "        \n",
        "        if self.theta_init is None:\n",
        "            theta_old = np.zeros((D + 1,))\n",
        "        else:\n",
        "            theta_old = self.theta_init\n",
        "\n",
        "        self.hist_theta_ = np.array([theta_old])\n",
        "        cost = self.compute_cost(theta_old, X, y)\n",
        "        self.hist_cost_ = np.array([cost])\n",
        "        \n",
        "        for i in range(self.max_iter):\n",
        "            grad = self.compute_gradient(theta_old, X, y)\n",
        "            theta_new = theta_old - self.alpha*grad\n",
        "            cost = self.compute_cost(theta_old, X, y)\n",
        "            self.hist_cost_ = np.vstack([self.hist_cost_, cost])\n",
        "            self.hist_theta_ = np.vstack([self.hist_theta_, theta_new])\n",
        "            if self.has_converged(theta_old, theta_new):\n",
        "                break\n",
        "            theta_old = theta_new\n",
        "        self.theta_ = theta_new\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        \"\"\"\n",
        "        Predict the target variable values for the data points in X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_hat: numpy.ndarray of shape (N,)\n",
        "            The predicted target variables values for the data points in X\n",
        "        \"\"\"\n",
        "\n",
        "        N = X.shape[0]\n",
        "        X = np.hstack((np.ones((N, 1)), X))\n",
        "        \n",
        "        return np.matmul(X,self.theta_)"
      ],
      "metadata": {
        "id": "x_iD4A-TmjKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2. Synthetic dataset**\n",
        "\n",
        "In this section we will first create some synthetic data on which we will run your linear regression implementation. We are creating 100 datapoints around the function y = mx + b, introducing Gaussian noise."
      ],
      "metadata": {
        "id": "hGlB3lojik_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 100\n",
        "np.random.seed(1)\n",
        "noise = np.random.randn(num_samples, 1)\n",
        "X = np.random.randn(num_samples, 1)\n",
        "\n",
        "y_ideal = 11*X + 5\n",
        "y_real = (11*X + 5) + noise\n",
        "\n",
        "plt.plot(X, y_real, 'ro')\n",
        "plt.plot(X, y_ideal, 'b')"
      ],
      "metadata": {
        "id": "Rqp2jLGTiJQo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "b6d7f7a3-ac17-4a90-8e8c-9270b691ac7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5d5e5cead0>]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IUC5Iryhvmqg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}